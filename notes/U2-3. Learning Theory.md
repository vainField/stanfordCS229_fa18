## **<u>L8. Bias-Variance tradeoff. Regularization and model/feature selection.</u>**

# ***Part VI. Learning Theory***

## **Bias/variance tradeoff**

- Informally, we define the **bias** of a model to be the expected generalization error even if we were to fit it to a very (say, infinitely) large training set.
- to be cont.



# ***Part VII. Regularization and model selection***

## **Cross validation**

- In **hold-out cross validation** (also called **simple cross validation**), we do the following:
  1. Randomly split $S$ into $S_{\text {train }}$ (say, $70 \%$ of the data) and $S_{\mathrm{cv}}$ (the remaining $30 \%)$. Here, $S_{\mathrm{cv}}$ is called the hold-out cross validation set.
  2. Train each model $M_{i}$ on $S_{\text {train }}$ only, to get some hypothesis $h_{i}$.
  3. Select and output the hypothesis $h_{i}$ that had the smallest error $\hat{\varepsilon}_{S_{\mathrm{cv}}}\left(h_{i}\right)$ on the hold out cross validation set. (Recall, $\hat{\varepsilon}_{S_{\mathrm{cv}}}(h)$ denotes the empirical error of $h$ on the set of examples in $S_{\mathrm{cv}}$.)
     - Optionally, step 3 in the algorithm may also be replaced with selecting the model $M_{i}$ according to $\arg \min _{i} \hat{\varepsilon}_{S_{\mathrm{cv}}}\left(h_{i}\right)$, and then retraining $M_{i}$ on the entire training set $S$.
     - The disadvantage of using hold out cross validation is that it "wastes" about $30 \%$ of the data.

- Here is a method, called **$k$-fold cross validation**, that holds out less data each time:
  1. Randomly split $S$ into $k$ disjoint subsets of $m / k$ training examples each. Let's call these subsets $S_{1}, \ldots, S_{k}$.
  2. For each model $M_{i}$, we evaluate it as follows:
     For $j=1, \ldots, k$
     Train the model $M_{i}$ on $S_{1} \cup \cdots \cup S_{j-1} \cup S_{j+1} \cup \cdots S_{k}$ (i.e., train on all the data except $S_{j}$ ) to get some hypothesis $h_{i j}$.
     Test the hypothesis $h_{i j}$ on $S_{j}$, to get $\hat{\varepsilon}_{S_{j}}\left(h_{i j}\right)$.
     The estimated generalization error of model $M_{i}$ is then calculated as the average of the $\hat{\varepsilon}_{S_{j}}\left(h_{i j}\right)$ 's (averaged over $j$ ).
  3. Pick the model $M_{i}$ with the lowest estimated generalization error, and retrain that model on the entire training set $S$. The resulting hypothesis is then output as our final answer.
     - A typical choice for the number of folds to use here would be $k=10$. 

- In problems in which data is really scarce, sometimes we will use the extreme choice of $k = m$ in order to leave out as little data as possible each time. this method is called **leave-one-out cross validation**.

## **Feature Selection**

- The following search procedure is called **forward search**:

  1. Initialize $\mathcal{F}=\emptyset$.
  2. Repeat \{
     (a) For $i=1, \ldots, n$ if $i \notin \mathcal{F}$, let $\mathcal{F}_{i}=\mathcal{F} \cup\{i\}$, and use some version of cross validation to evaluate features $\mathcal{F}_{i}$. (I.e., train your learning algorithm using only the features in $\mathcal{F}_{i}$, and estimate its generalization error.)
     (b) Set $\mathcal{F}$ to be the best feature subset found on step (a). 
     \}
  3. Select and output the best feature subset that was evaluated during the entire search procedure.
     - This algorithm described above one instantiation of **wrapper model feature selection**. Besides, **backward search** starts off with $\mathcal{F}=\{1, \ldots, n\}$ as the set of all features, and repeatedly deletes features one at a time until $\mathcal{F}=\emptyset$.
     - Complete forward search (terminating when $\mathcal{F}=\{1, \ldots, n\})$ would take about $O\left(n^{2}\right)$ calls to the learning algorithm.

- **Filter feature selection** methods is to compute some simple score $S(i)$ that measures how informative each feature $x_{i}$ is about the class labels $y$. Then, we simply pick the $k$ features with the largest scores $S(i)$.

  - In practice, it is common (particularly for discrete-valued features $\left.x_{i}\right)$ to choose $S(i)$ to be the **mutual information** $\operatorname{MI}\left(x_{i}, y\right)$ between $x_{i}$ and $y$ :
    $$
    \operatorname{MI}\left(x_{i}, y\right)=\sum_{x_{i} \in\{0,1\}} \sum_{y \in\{0,1\}} p\left(x_{i}, y\right) \log \frac{p\left(x_{i}, y\right)}{p\left(x_{i}\right) p(y)}
    $$

    - see Notes5 for detail

## **Bayesian statistics and regularization**

- see Notes5 for detail


