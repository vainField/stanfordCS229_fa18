# **<u>L1. Introduction and Basic Concepts</u>**

### Course Content


1. Supervised learning
   - Problem Types
     - Regression
     - Classification
2. Machine Learning Strategy (Learning Theory)
3. Deep Learning
4. Unsupervised Learning
5. Reinforcement Learning

# **<u>L2. Supervised Learning Setup. Linear Regression</u>**

# **<u>Supervised learning</u>**

<img src="image.assets/Screen Shot 2022-03-25 at 17.20.48.png" alt="Screen Shot 2022-03-25 at 17.20.48" style="zoom:20%;" />

### Notation

- $x^{(i)}$ to denote the “input” variables, also called input **features**
- $y^{(i)}$ to denote the “output” or **target** variable that we are trying to predict
- A pair $(x^{(i)} , y^{(i)} )$ is called a **training example**
- The dataset that we’ll be using to learn—a list of m training examples $\{(x^{(i)} , y^{(i)});\ i = 1, . . . , m\}$—is called a **training set**
  - Note that the superscript “$(i)$” in the notation is simply an index into the training set
  - We will also use $\mathcal{X}$ denote the space of input values, and $\mathcal{Y}$ the space of output values. In this example, $\mathcal{X}=\mathcal{Y}=\mathbb{R}$
- Our goal is, given a training set, to learn a function $h: \mathcal{X} \mapsto \mathcal{Y}$ so that $h(x)$ is a "good" predictor for the corresponding value of $y$. For historical reasons, this function $h$ is called a **hypothesis**.
  - When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a **regression** problem. 
  - When $y$ can take on only a small number of discrete values, we call it a **classification** problem.

# **<u>Part I Linear Regression</u>**

## **Linear Regression**

- Let’s consider a slightly richer dataset in which we also know the number of bedrooms in each house

- As an initial choice, let’s say we decide to approximate $y$ as a linear function of $x$:
  $$
  h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}
  $$

  - Here, the $\theta_{i}$ 's are the **parameters** (also called **weights**) parameterizing the space of linear functions mapping from $\mathcal{X}$ to $\mathcal{Y}$. 

- To simplify our notation, we introduce the convention of letting $x_{0}=1$ (this is the intercept term), so that
  $$
  h(x)=\sum_{i=0}^{n} \theta_{i} x_{i}=\theta^{T} x
  $$

  - here $n$ is the number of input variables (not counting $x_{0}$ ).

- how do we pick the parameters $\theta$ ? One reasonable method seems to be to make $h(x)$ close to $y$. To formalize this, We define the **cost function**:
  $$
  J(\theta)=\frac{1}{2} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
  $$

### LMS algorithm (least mean squares)

- Let’s consider the **gradient descent** algorithm, which starts with some initial θ, and repeatedly performs the update:
  $$
  J(\theta)=\frac{1}{2} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
  $$

  - Here, $\alpha$ is called the learning rate. This is a very natural algorithm that repeatedly takes a step in the direction of steepest decrease of $J$.

- We have to work out what is the partial derivative term on the right hand side. Let's first work it out if we have only one training example $(x, y)$
  $$
  \begin{aligned}
  \frac{\partial}{\partial \theta_{j}} J(\theta) &=\frac{\partial}{\partial \theta_{j}} \frac{1}{2}\left(h_{\theta}(x)-y\right)^{2} \\
  &=2 \cdot \frac{1}{2}\left(h_{\theta}(x)-y\right) \cdot \frac{\partial}{\partial \theta_{j}}\left(h_{\theta}(x)-y\right) \\
  &=\left(h_{\theta}(x)-y\right) \cdot \frac{\partial}{\partial \theta_{j}}\left(\sum_{i=0}^{n} \theta_{i} x_{i}-y\right) \\
  &=\left(h_{\theta}(x)-y\right) x_{j}
  \end{aligned}
  $$

  - For a single training example, this gives the update rule:
    $$
    \theta_{j}:=\theta_{j}+\alpha\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}
    $$

    - The rule is called the **LMS** update rule (LMS stands for “least mean squares”), and is also known as the **Widrow-Hoff** learning rule.
    - Note that <u>*the magnitude of the update is proportional to the error term*</u>

- **Batch gradient descent**
  $$
  \begin{align}
  &\text{Repeat until convergence }\{ \\
  &\qquad \theta_{j}:=\theta_{j}+\alpha \sum_{i=1}^{m}\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)} \quad (\text{for every } j) \\
  &\}
  \end{align}
  $$

- **Stochastic gradient descent** (also **incremental gradient descent**)
  $$
  \begin{align}
  &\text{Loop }\{ \\
  &\qquad \text{for } i = 1 \text{ to } m,\ \{ \\
  &\qquad \qquad \theta_{j}:=\theta_{j}+\alpha\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)} \quad (\text{for every } j) \\
  &\qquad \} \\
  &\}
  \end{align}
  $$

  - (Note however that it may never “converge” to the minimum, and the parameters $\theta$ will keep oscillating around the minimum of $J(\theta)$; but in practice most of the values near the minimum will be reasonably good approximations to the true minimum.)

## **The normal equations**

- In this method, we will minimize $J$ by explicitly taking its derivatives with respect to the $\theta_j$’s, and setting them to zero.

### Matrix derivatives

- For a function $f: \mathbb{R}^{m \times n} \mapsto \mathbb{R}$ mapping from $m$-by-n matrices to the real numbers, we define the derivative of $f$ with respect to $A$ to be:
  $$
  \nabla_{A} f(A)=\left[\begin{array}{ccc}
  \frac{\partial f}{\partial A_{11}} & \cdots & \frac{\partial f}{\partial A_{1 n}} \\
  \vdots & \ddots & \vdots \\
  \frac{\partial f}{\partial A_{m 1}} & \cdots & \frac{\partial f}{\partial A_{m n}}
  \end{array}\right]
  $$

- For an $n$-by- $n$ (square) matrix $A$, the trace of $A$ is defined to be the sum of its diagonal entries:
  $$
  \operatorname{tr} A=\sum_{i=1}^{n} A_{i i}
  $$

  - If $a$ is a real number (i.e., a 1-by-1 matrix), then $\operatorname{tr} a=a$.

  - The trace operator has the property that for two matrices $A$ and $B$ such that $A B$ is square, we have that $\operatorname{tr} A B=\operatorname{tr} B A$.

  - The following properties of the trace operator are also easily verified. Here, $A$ and $B$ are square matrices, and $a$ is a real number:
    $$
    \begin{aligned}
    \operatorname{tr} A &=\operatorname{tr} A^{T} \\
    \operatorname{tr}(A+B) &=\operatorname{tr} A+\operatorname{tr} B \\
    \operatorname{tr} a A &=a \operatorname{tr} A
    \end{aligned}
    $$

- We now state without proof some facts of matrix derivatives (we won't need some of these until later this quarter). Equation $(4)$ applies only to non-singular square matrices $A$, where $|A|$ denotes the determinant of $A$. We have:
  $$
  \begin{aligned}
  \nabla_{A} \operatorname{tr} A B &=B^{T} &(1)\\
  \nabla_{A^{T}} f(A) &=\left(\nabla_{A} f(A)\right)^{T} &(2)\\
  \nabla_{A} \operatorname{tr} A B A^{T} C &=C A B+C^{T} A B^{T} &(3)\\
  \nabla_{A}|A| &=|A|\left(A^{-1}\right)^{T} &(4)
  \end{aligned}
  $$

### Least squares revisited

- Using the fact that for a vector $z$, we have that $z^{T} z=\sum_{i} z_{i}^{2}$ :
  $$
  \begin{aligned}
  \frac{1}{2}(X \theta-\vec{y})^{T}(X \theta-\vec{y}) &=\frac{1}{2} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} \\
  &=J(\theta)
  \end{aligned}
  $$

- To minimize $J$, let's find its derivatives with respect to $\theta$. Combining Equations (2) and (3), we find that
  $$
  \nabla_{A^{T} \operatorname{tr}} A B A^{T} C=B^{T} A^{T} C^{T}+B A^{T} C
  $$

- Hence,
  $$
  \begin{aligned}
  \nabla_{\theta} J(\theta) &=\nabla_{\theta} \frac{1}{2}(X \theta-\vec{y})^{T}(X \theta-\vec{y}) \\
  &=\frac{1}{2} \nabla_{\theta}\left(\theta^{T} X^{T} X \theta-\theta^{T} X^{T} \vec{y}-\vec{y}^{T} X \theta+\vec{y}^{T} \vec{y}\right) \\
  &=\frac{1}{2} \nabla_{\theta} \operatorname{tr}\left(\theta^{T} X^{T} X \theta-\theta^{T} X^{T} \vec{y}-\vec{y}^{T} X \theta+\vec{y}^{T} \vec{y}\right) \\
  &=\frac{1}{2} \nabla_{\theta}\left(\operatorname{tr} \theta^{T} X^{T} X \theta-2 \operatorname{tr} \vec{y}^{T} X \theta\right) \\
  &=\frac{1}{2}\left(X^{T} X \theta+X^{T} X \theta-2 X^{T} \vec{y}\right) \\
  &=X^{T} X \theta-X^{T} \vec{y}
  \end{aligned}
  $$

- To minimize $J$, we set its derivatives to zero, and obtain the **normal equations**:
  $$
  X^{T} X \theta=X^{T} \vec{y}
  $$

- Thus, the value of $\theta$ that minimizes $J(\theta)$ is given in closed form by the equation
  $$
  \theta=\left(X^{T} X\right)^{-1} X^{T} \vec{y}
  $$

  - if $X^{T} X$ is not invertible, it's probably because $X$ has some repeated features (can also just use pseudo inverse)

## *Thinking*

- 通过notation对监督学习的定义
- 假设hypothesis $h$为一个线性方程
  - --》使用差平方定义cost function
    - --》使用随机梯度下降找到参数$\theta$
  - 该线性方程为convex方程，可以通过使梯度为0直接找到最优解
    - 定义了一些矩阵运算的符号和公式
    - 推导出normal equation来计算参数的最优解



# **<u>L3. Weighted Least Squares. Logistic Regression. Netwon's Method</u>**

## **Probabilistic interpretation**

- Why might linear regression, and specifically why might the least-squares cost function J, be a reasonable choice?

- Let us assume that the target variables and the inputs are related via the equation
  $$
  y^{(i)}=\theta^{T} x^{(i)}+\epsilon^{(i)}
  $$

  - where $\epsilon^{(i)}$ is an error term that captures either unmodeled effects, or random noise
  - Let us further assume that the $\epsilon^{(i)}$ are distributed **IID** and $\epsilon^{(i)} \sim \mathcal{N}\left(0, \sigma^{2}\right)$

- the density of $\epsilon^{(i)}$ is given by
  $$
  p\left(\epsilon^{(i)}\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(\epsilon^{(i)}\right)^{2}}{2 \sigma^{2}}\right)
  $$

  - This implies that
    $$
    p\left(y^{(i)} \mid x^{(i)} ; \theta\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)
    $$

  - The notation "$p\left(y^{(i)} \mid x^{(i)} ; \theta\right)$" indicates that this is the distribution of $y^{(i)}$ given $x^{(i)}$ and parameterized by $\theta$. 

  - We can also write the distribution of $y^{(i)}$ as
    $$
    y^{(i)} \mid x^{(i)} ; \theta \sim \mathcal{N}\left(\theta^{T} x^{(i)}, \sigma^{2}\right)
    $$

- When we wish to explicitly view this as a function of $\theta$, we will instead call it the **likelihood** function:
  $$
  \begin{aligned}
  L(\theta)&=L(\theta ; X, \vec{y})=p(\vec{y} \mid X ; \theta)\\
  &=\prod_{i=1}^{m} p\left(y^{(i)} \mid x^{(i)} ; \theta\right) \\
  &=\prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)
  \end{aligned}
  $$

- The principal of **maximum likelihood** says that we should choose $\theta$ so as to make the data as high probability as possible

- Instead of maximizing $L(\theta)$, *<u>we can also maximize any strictly increasing function of $L(\theta)$​.</u>* In particular, we instead maximize the **log likelihood** $\ell(\theta)$ :
  $$
  \begin{aligned}
  \ell(\theta) &=\log L(\theta) \\
  &=\log \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right) \\
  &=\sum_{i=1}^{m} \log \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right) \\
  &=m \log \frac{1}{\sqrt{2 \pi} \sigma}-\frac{1}{\sigma^{2}} \cdot \frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}
  \end{aligned}
  $$

  - Hence, maximizing $\ell(\theta)$ gives the same answer as minimizing

  $$
  \frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}
  $$

  - which we recognize to be $J(\theta)$, our original least-squares cost function.
  - (Note however that the probabilistic assumptions are by no means necessary for least-squares to be a perfectly good and rational procedure, and there may—and indeed there are—other natural assumptions that can also be used to justify it.)

## **Locally weighted linear regression (LWR)**

- In the original linear regression algorithm, to make a prediction at a query point $x$ (i.e., to evaluate $h(x)$), we would:

  1. Fit $\theta$ to minimize $\sum_{i}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}$.
  2. Output $\theta^{T} x$.

- In contrast, the locally weighted linear regression algorithm does the following:

  1. Fit $\theta$ to minimize $\sum_{i} w^{(i)}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}$.

  2. Output $\theta^{T} x$.

     - Here, the $w^{(i)}$ 's are non-negative valued **weights**. Intuitively, 

       - if $w^{(i)}$ is large for a particular value of $i$, then in picking $\theta$, we'll try hard to make $\left(y^{(i)}-\right.$ $\left.\theta^{T} x^{(i)}\right)^{2}$ small. 
       - If $w^{(i)}$ is small, then the $\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}$ error term will be pretty much ignored in the fit.

     - A fairly standard choice for the weights is
       $$
       w^{(i)}=\exp \left(-\frac{\left(x^{(i)}-x\right)^{2}}{2 \tau^{2}}\right)
       $$

       - Note that the weights depend on the particular point $x$ at which we're trying to evaluate $x$.
       - The parameter $\tau$ controls how quickly the weight of a training example falls off with distance of its $x^{(i)}$ from the query point $x ; \tau$ is called the **bandwidth** parameter

- Locally weighted linear regression is the first example we’re seeing of a **non-parametric** algorithm,

  - (roughly) refers to the fact that the amount of stuff we need to keep in order to represent the hypothesis h grows linearly with the size of the training set.







