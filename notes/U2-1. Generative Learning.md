# ***Part IV. Generative Learning algorithms***

## **<u>L5. Gaussian Discriminant Analysis. Naive Bayes.</u>**

## **Generative Learning algorithms**

- Algorithms that try to <u>*learn $p(y \mid x)$ directly*</u> (such as logistic regression), or algorithms that try to learn mappings directly from the space of inputs $\mathcal{X}$ to the labels $\{0,1\}$, (such as the perceptron algorithm) are called **discriminative learning algorithms**. 

- Here, we'll talk about algorithms that instead try to <u>*model $p(x \mid y)$ (and $p(y)$ ).*</u> These algorithms are called **generative learning algorithms**.

  - After modeling $p(y)$ (called the class priors) and $p(x \mid y)$, our algorithm can then use Bayes rule to derive the posterior distribution on $y$ given $x$ :
    $$
    p(y \mid x)=\frac{p(x \mid y) p(y)}{p(x)}
    $$

    - Here, the denominator is given by $p(x)=p(x \mid y=1) p(y=1)+p(x \mid y=0) p(y=0)$

    - Actually, if were calculating $p(y \mid x)$ in order to make a prediction, then we don't actually need to calculate the denominator, since
      $$
      \begin{aligned}
      \arg \max _{y} p(y \mid x) &=\arg \max _{y} \frac{p(x \mid y) p(y)}{p(x)} \\
      &=\arg \max _{y} p(x \mid y) p(y)
      \end{aligned}
      $$

## **Gaussian discriminant analysis (GDA)**

### The multivariate normal distribution

- The **multivariate normal distribution** in $n$-dimensions, also called the **multivariate Gaussian distribution**, is parameterized by a **mean vector** $\mu \in \mathbb{R}^{n}$ and a **covariance matrix** $\Sigma \in \mathbb{R}^{n \times n}$, where $\Sigma \geq 0$ is **symmetric** and **positive semi-definite**. Also written " $\mathcal{N}(\mu, \Sigma)$ ", its density is given by:
  $$
  p(x ; \mu, \Sigma)=\frac{1}{(2 \pi)^{n / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)
  $$

  - In the equation above, "$|\Sigma|$" denotes the **determinant** of the matrix $\Sigma$.

  - For a random variable $X$ distributed $\mathcal{N}(\mu, \Sigma)$, the **mean** is (unsurprisingly) given by $\mu$ :
    $$
    \mathrm{E}[X]=\int_{x} x p(x ; \mu, \Sigma) d x=\mu
    $$

  - The **covariance** of a vector-valued random variable $Z$ is defined as $\operatorname{Cov}(Z)=\mathrm{E}\left[(Z-\mathrm{E}[Z])(Z-\mathrm{E}[Z])^{T}\right]$. This generalizes the notion of the variance of a real-valued random variable. The covariance can also be defined as $\operatorname{Cov}(Z)=\mathrm{E}\left[Z Z^{T}\right]-(\mathrm{E}[Z])(\mathrm{E}[Z])^{T}$. If $X \sim \mathcal{N}(\mu, \Sigma)$, then
    $$
    \operatorname{Cov}(X)=\Sigma \text {. }
    $$

### The Gaussian Discriminant Analysis model

- When we have <u>*a classification problem in which the input features $x$ are continuous-valued random variables*</u>, we can then use the GDA model, which models $p(x \mid y)$ using a multivariate normal distribution:
  $$
  \begin{aligned}
  y & \sim \operatorname{Bernoulli}(\phi) \\
  x \mid y=0 & \sim \mathcal{N}\left(\mu_{0}, \Sigma\right) \\
  x \mid y=1 & \sim \mathcal{N}\left(\mu_{1}, \Sigma\right)
  \end{aligned}
  $$

  - Writing out the distributions, this is:
    $$
    \begin{aligned}
    p(y) &=\phi^{y}(1-\phi)^{1-y} \\
    p(x \mid y=0) &=\frac{1}{(2 \pi)^{n / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x-\mu_{0}\right)^{T} \Sigma^{-1}\left(x-\mu_{0}\right)\right) \\
    p(x \mid y=1) &=\frac{1}{(2 \pi)^{n / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x-\mu_{1}\right)^{T} \Sigma^{-1}\left(x-\mu_{1}\right)\right)
    \end{aligned}
    $$

    - Here, the parameters of our model are $\phi, \Sigma, \mu_{0}$ and $\mu_{1}$.

  - The log-likelihood of the data is given by
    $$
    \begin{aligned}
    \ell\left(\phi, \mu_{0}, \mu_{1}, \Sigma\right) &=\log \prod_{i=1}^{m} p\left(x^{(i)}, y^{(i)} ; \phi, \mu_{0}, \mu_{1}, \Sigma\right) \\
    &=\log \prod_{i=1}^{m} p\left(x^{(i)} \mid y^{(i)} ; \mu_{0}, \mu_{1}, \Sigma\right) p\left(y^{(i)} ; \phi\right)
    \end{aligned}
    $$

  - By maximizing $\ell$ with respect to the parameters, we find the maximum likelihood estimate of the parameters to be:
    $$
    \begin{aligned}
    \phi &=\frac{1}{m} \sum_{i=1}^{m} 1\left\{y^{(i)}=1\right\} \\
    \mu_{0} &=\frac{\sum_{i=1}^{m} 1\left\{y^{(i)}=0\right\} x^{(i)}}{\sum_{i=1}^{m} 1\left\{y^{(i)}=0\right\}} \\
    \mu_{1} &=\frac{\sum_{i=1}^{m} 1\left\{y^{(i)}=1\right\} x^{(i)}}{\sum_{i=1}^{m} 1\left\{y^{(i)}=1\right\}} \\
    \Sigma &=\frac{1}{m} \sum_{i=1}^{m}\left(x^{(i)}-\mu_{y^{(i)}}\right)\left(x^{(i)}-\mu_{y^{(i)}}\right)^{T} .
    \end{aligned}
    $$

### Discussion: GDA and logistic regression

- If we view the quantity $p\left(y=1 \mid x ; \phi, \mu_{0}, \mu_{1}, \Sigma\right)$ as a function of $x$, we'll find that the GDA model can be expressed in the form
  $$
  p\left(y=1 \mid x ; \phi, \Sigma, \mu_{0}, \mu_{1}\right)=\frac{1}{1+\exp \left(-\theta^{T} x\right)},
  $$

  - where $\theta$ is some appropriate function of $\phi, \Sigma, \mu_{0}, \mu_{1}$ 

- We just argued that *<u>if $p(x \mid y)$ is multivariate gaussian (with shared $\Sigma$ ), then $p(y \mid x)$ necessarily follows a logistic function</u>*. 

  - However, *<u>$p(y \mid x)$ being a logistic function does not imply $p(x \mid y)$ is multivariate gaussian</u>*.

- This shows that GDA makes <u>*stronger modeling assumptions*</u> about the data than does logistic regression.

  - It turns out that when these modeling assumptions are correct, then GDA will find better fits to the data, and is **asymptotically efficient**.
  - In contrast, by making significantly weaker assumptions, logistic regression is also **more robust** and <u>*less sensitive to incorrect modeling assumptions*</u>.
    - There are many different sets of assumptions that would lead to $p(y \mid x)$ taking the form of a logistic function. For example, if $x \mid y=0 \sim \operatorname{Poisson}\left(\lambda_{0}\right)$, and $x \mid y=1 \sim \operatorname{Poisson}\left(\lambda_{1}\right)$, then $p(y \mid x)$ will be logistic.

- To summarize: 

  - GDA makes stronger modeling assumptions, and is more data efficient when the modeling assumptions are correct or at least approximately correct. 
  - Logistic regression makes weaker assumptions, and is significantly more robust to deviations from modeling assumptions.

## **Naive Bayes**

- Let’s now talk about a different learning algorithm in which the $x_i$’s are <u>*discrete-valued*</u>. For our motivating example, consider building an email spam filter.

- if an email contains the $i$-th word of the dictionary, then we will set $x_{i}=1$; otherwise, we let $x_{i}=0$. For instance, the vector
  $$
  x=\left[\begin{array}{c}
  1 \\
  0 \\
  0 \\
  \vdots \\
  1 \\
  \vdots \\
  0
  \end{array}\right] \quad \begin{aligned}
  &\text { a } \\
  &\text { aardvark } \\
  &\text { aardwolf } \\
  &\vdots \\
  &\text { buy } \\
  &\vdots \\
  &\text { zygmurgy }
  \end{aligned}
  $$
  is used to represent an email that contains the words "a" and "buy"

- **Naive Bayes (NB) assumption**

  - If we have a vocabulary of 50000 words, and if we were to model $x$ explicitly with a multinomial distribution over the $2^{50000}$ possible outcomes, then we'd end up with a $(2^{50000}-1)$ dimensional parameter vector.
  - To model $p(x \mid y)$, we will therefore assume that the $x_{i}$ 's are conditionally independent given $y$. This assumption is called the Naive Bayes (NB) assumption, and the resulting algorithm is called the **Naive Bayes classifier**.

- We now have:
  $$
  \begin{aligned}
  p &\left(x_{1}, \ldots, x_{50000} \mid y\right) \\
  \quad &=p\left(x_{1} \mid y\right) p\left(x_{2} \mid y, x_{1}\right) p\left(x_{3} \mid y, x_{1}, x_{2}\right) \cdots p\left(x_{50000} \mid y, x_{1}, \ldots, x_{49999}\right) \\
  &=p\left(x_{1} \mid y\right) p\left(x_{2} \mid y\right) p\left(x_{3} \mid y\right) \cdots p\left(x_{50000} \mid y\right) \\
  &=\prod_{i=1}^{n} p\left(x_{i} \mid y\right)
  \end{aligned}
  $$

  - Even though the Naive Bayes assumption is an extremely strong assumptions, the resulting algorithm works well on many problems.

- Our model is parameterized by $\phi_{i \mid y=1}=p\left(x_{i}=1 \mid y=1\right), \phi_{i \mid y=0}=p\left(x_{i}=\right.$ $1 \mid y=0)$, and $\phi_{y}=p(y=1)$. We can write down the joint likelihood of the data:
  $$
  \mathcal{L}\left(\phi_{y}, \phi_{j \mid y=0}, \phi_{j \mid y=1}\right)=\prod_{i=1}^{m} p\left(x^{(i)}, y^{(i)}\right)
  $$

  - Maximizing this with respect to $\phi_{y}, \phi_{i \mid y=0}$ and $\phi_{i \mid y=1}$ gives the maximum likelihood estimates:

  $$
  \begin{aligned}
  \phi_{j \mid y=1} &=\frac{\sum_{i=1}^{m} 1\left\{x_{j}^{(i)}=1 \wedge y^{(i)}=1\right\}}{\sum_{i=1}^{m} 1\left\{y^{(i)}=1\right\}} \\
  \phi_{j \mid y=0} &=\frac{\sum_{i=1}^{m} 1\left\{x_{j}^{(i)}=1 \wedge y^{(i)}=0\right\}}{\sum_{i=1}^{m} 1\left\{y^{(i)}=0\right\}} \\
  \phi_{y} &=\frac{\sum_{i=1}^{m} 1\left\{y^{(i)}=1\right\}}{m}
  \end{aligned}
  $$

- Having fit all these parameters, to make a prediction on a new example with features $x$, we then simply calculate
  $$
  \begin{aligned}
  p(y=1 \mid x) &=\frac{p(x \mid y=1) p(y=1)}{p(x)} \\
  &=\frac{\left(\prod_{i=1}^{n} p\left(x_{i} \mid y=1\right)\right) p(y=1)}{\left(\prod_{i=1}^{n} p\left(x_{i} \mid y=1\right)\right) p(y=1)+\left(\prod_{i=1}^{n} p\left(x_{i} \mid y=0\right)\right) p(y=0)}
  \end{aligned}
  $$
  and pick whichever class has the higher posterior probability.

### In Lecture

- <u>*When you have less data, the algorithm needs to rely more on assumptions you code in, and your skill at coding and your knowledge matters much more*</u>

## *Thinking*

- 判别模型 vs. 生成模型
  - [Jebara (2004)](https://en.wikipedia.org/wiki/Generative_model#CITEREFJebara2004):
    1. A generative model is a statistical model of the joint probability distribution $P(X, Y)$ on given observable variable $X$ and target variable $Y ;[1]$
    2. A discriminative model is a model of the conditional probability $P(Y \mid X=x)$ of the target $Y$, given an observation $x$; and
  - 生成模型（如GDA）通常有更强的假设（如$p(x;y)$为多元高斯分布），因此当假设正确时，需要的数据量更小且模型的准确度更高；
  - 而判别模型由于较弱的假设（如$p(y;x)$为逻辑函数），通常更牢靠（robust）

## **<u>L6. Laplace Smoothing. Support Vector Machines.</u>**

### Laplace smoothing

- it is statistically a bad idea to estimate the probability of some event to be zero just because you haven’t seen it before.

- To avoid this, we can use **Laplace smoothing**, which replaces the above estimate with
  $$
  \phi_{j}=\frac{\sum_{i=1}^{m} 1\left\{z^{(i)}=j\right\}+1}{m+k} .
  $$

  - Here, we've added 1 to the numerator, and $k$ to the denominator.

### Event models for text classification

- In the specific context of text classification, Naive Bayes as presented uses the what’s called the **multi-variate Bernoulli event model**.

- Here’s a different model, called the **multinomial event model**. To describe this model, we will use a different notation and set of features.

  - We let $x_{i}$ denote the identity of the $i$-th word in the email. Thus, $x_{i}$ is now an integer taking values in $\{1, \ldots,|V|\}$, where $|V|$ is the size of our vocabulary (dictionary). 
  - An email of $n$ words is now represented by a vector $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ of length $n$; note that $n$ can vary for different documents.

- If we are given a training set $\left\{\left(x^{(i)}, y^{(i)}\right) ; i=1, \ldots, m\right\}$ where $x^{(i)}=$ $\left(x_{1}^{(i)}, x_{2}^{(i)}, \ldots, x_{n_{i}}^{(i)}\right)$ (here, $n_{i}$ is the number of words in the $i$-training example), the likelihood of the data is given by
  $$
  \begin{aligned}
  \mathcal{L}\left(\phi, \phi_{k \mid y=0}, \phi_{k \mid y=1}\right) &=\prod_{i=1}^{m} p\left(x^{(i)}, y^{(i)}\right) \\
  &=\prod_{i=1}^{m}\left(\prod_{j=1}^{n_{i}} p\left(x_{j}^{(i)} \mid y ; \phi_{k \mid y=0}, \phi_{k \mid y=1}\right)\right) p\left(y^{(i)} ; \phi_{y}\right)
  \end{aligned}
  $$

- Maximizing this yields the maximum likelihood estimates of the parameters:
  $$
  \begin{aligned}
  \phi_{k \mid y=1} &=\frac{\sum_{i=1}^{m} \sum_{j=1}^{n_{i}} 1\left\{x_{j}^{(i)}=k \wedge y^{(i)}=1\right\}}{\sum_{i=1}^{m} 1\left\{y^{(i)}=1\right\} n_{i}} \\
  \phi_{k \mid y=0} &=\frac{\sum_{i=1}^{m} \sum_{j=1}^{n_{i}} 1\left\{x_{j}^{(i)}=k \wedge y^{(i)}=0\right\}}{\sum_{i=1}^{m} 1\left\{y^{(i)}=0\right\} n_{i}} \\
  \phi_{y} &=\frac{\sum_{i=1}^{m} 1\left\{y^{(i)}=1\right\}}{m}
  \end{aligned}
  $$

- While not necessarily the very best classification algorithm, the Naive Bayes classifier often works surprisingly well. It is often also a very good "first thing to try," given its simplicity and ease of implementation.





















