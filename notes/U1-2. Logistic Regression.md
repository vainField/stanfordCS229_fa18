# **<u>L3. cont.</u>**

# ***Part II Classification and logistic regression***

### Basic Setup

- For now, we will focus on the binary classification problem in which $y$ can take on only two values, $0$ and $1$

  - 0 is also called the **negative class**, and 1 the **positive class**, and they are sometimes also denoted by the symbols "-" and "+." 
  - Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the **label** for the training example.

- let's change the form for our hypotheses $h_{\theta}(x)$. We will choose
  $$
  h_{\theta}(x)=g\left(\theta^{T} x\right)=\frac{1}{1+e^{-\theta^{T} x}},
  $$
  where
  $$
  g(z)=\frac{1}{1+e^{-z}}
  $$
  is called the **logistic function** or the **sigmoid function**.

  <img src="image.assets/Screen Shot 2022-03-26 at 12.14.54.png" alt="Screen Shot 2022-03-26 at 12.14.54" style="zoom:33%;" />

  - Before moving on, here’s a useful property of the derivative of the sigmoid function, which we write as $g\prime$:
    $$
    \begin{aligned}
    g^{\prime}(z) &=\frac{d}{d z} \frac{1}{1+e^{-z}} \\
    &=\frac{1}{\left(1+e^{-z}\right)^{2}}\left(e^{-z}\right) \\
    &=\frac{1}{\left(1+e^{-z}\right)} \cdot\left(1-\frac{1}{\left(1+e^{-z}\right)}\right) \\
    &=g(z)(1-g(z))
    \end{aligned}
    $$

### Fit the parameters

- Let’s endow our classification model with a set of probabilistic assumptions, and then fit the parameters via maximum likelihood.

- Let us assume that
  $$
  \begin{aligned}
  &P(y=1 \mid x ; \theta)=h_{\theta}(x) \\
  &P(y=0 \mid x ; \theta)=1-h_{\theta}(x)
  \end{aligned}
  $$

- Note that this can be written more compactly as
  $$
  p(y \mid x ; \theta)=\left(h_{\theta}(x)\right)^{y}\left(1-h_{\theta}(x)\right)^{1-y}
  $$

- Assuming that the $m$ training examples were generated independently, we can then write down the likelihood of the parameters as
  $$
  \begin{aligned}
  L(\theta) &=p(\vec{y} \mid X ; \theta) \\
  &=\prod_{i=1}^{m} p\left(y^{(i)} \mid x^{(i)} ; \theta\right) \\
  &=\prod_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)\right)^{y^{(i)}}\left(1-h_{\theta}\left(x^{(i)}\right)\right)^{1-y^{(i)}}
  \end{aligned}
  $$

- As before, it will be easier to maximize the <u>*log likelihood*</u>:
  $$
  \begin{aligned}
  \ell(\theta) &=\log L(\theta) \\
  &=\sum_{i=1}^{m} y^{(i)} \log h\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h\left(x^{(i)}\right)\right)
  \end{aligned}
  $$

- Written in vectorial notation, our updates will therefore be given by $\theta:=\theta+\alpha \nabla_{\theta} \ell(\theta)$. Let's start by working with just one training example $(x, y)$, and take derivatives to derive the stochastic gradient ascent rule:
  $$
  \begin{aligned}
  \frac{\partial}{\partial \theta_{j}} \ell(\theta) &=\left(y \frac{1}{g\left(\theta^{T} x\right)}-(1-y) \frac{1}{1-g\left(\theta^{T} x\right)}\right) \frac{\partial}{\partial \theta_{j}} g\left(\theta^{T} x\right) \\
  &=\left(y \frac{1}{g\left(\theta^{T} x\right)}-(1-y) \frac{1}{1-g\left(\theta^{T} x\right)}\right) g\left(\theta^{T} x\right)\left(1-g\left(\theta^{T} x\right) \frac{\partial}{\partial \theta_{j}} \theta^{T} x\right.\\
  &=\left(y\left(1-g\left(\theta^{T} x\right)\right)-(1-y) g\left(\theta^{T} x\right)\right) x_{j} \\
  &=\left(y-h_{\theta}(x)\right) x_{j}
  \end{aligned}
  $$

- This therefore gives us the stochastic gradient ascent rule
  $$
  \theta_{j}:=\theta_{j}+\alpha\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}
  $$

  - If we compare this to the LMS update rule, we see that it looks identical

### Digression: The perceptron learning algorithm

- Consider modifying the logistic regression method to “force” it to output values that are either 0 or 1 or exactly. It seems natural to change the definition of $g$ to be the threshold function:
  $$
  g(z)= \begin{cases}1 & \text { if } z \geq 0 \\ 0 & \text { if } z<0\end{cases}
  $$

- If we then let $h_{\theta}(x)=g\left(\theta^{T} x\right)$ as before but using this modified definition of $g$, and if we use the update rule
  $$
  \theta_{j}:=\theta_{j}+\alpha\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}
  $$

  - then we have the **perceptron learning algorithm**
  - In the 1960s, this "perceptron" was argued to be a rough model for how individual neurons in the brain work. 

## **Newton’s method**

- To get us started, let's consider **Newton's method** for finding a zero of a function. Specifically, suppose we have some function $f: \mathbb{R} \mapsto \mathbb{R}$, and we wish to find a value of $\theta$ so that $f(\theta)=0$. Here, $\theta \in \mathbb{R}$ is a real number. Newton's method performs the following update:
  $$
  \theta:=\theta-\frac{f(\theta)}{f^{\prime}(\theta)}
  $$

- By letting $f(\theta)=\ell^{\prime}(\theta)$, we can use the same algorithm to maximize $\ell$, and we obtain update rule:
  $$
  \theta:=\theta-\frac{\ell^{\prime}(\theta)}{\ell^{\prime \prime}(\theta)}
  $$

- The generalization of Newton's method to this multidimensional setting (also called the Newton-Raphson method) is given by
  $$
  \theta:=\theta-H^{-1} \nabla_{\theta} \ell(\theta)
  $$

  - Here, $\nabla_{\theta} \ell(\theta)$ is, as usual, the vector of partial derivatives of $\ell(\theta)$ with respect to the $\theta_{i}$ 's; and $H$ is an $n$-by- $n$ matrix called the **Hessian**, whose entries are given by
    $$
    H_{i j}=\frac{\partial^{2} \ell(\theta)}{\partial \theta_{i} \partial \theta_{j}}
    $$

  - Newton’s method typically enjoys faster convergence. 

  - One iteration of Newton’s can, however, be more expensive, since it requires finding and inverting an $n$-by- $n$ Hessian; but so long as $n$ is not too large, it is usually much faster overall.

- When Newton's method is applied to maximize the logistic regression log likelihood function $\ell(\theta)$, the resulting method is also called **Fisher scoring**.

## *Thinking*

- 用概率论/数学公式精确地说明了在何种前提下选择相应的cost function是自然而然的结果（虽然该前提是充分不必要的）
  - 例如假设线性回归中的噪音是iid的且服从正态分布，通过概率解读及公式推导，得出最小均方函数是自然的损失/成本函数
- 简单讲解了Newton's Method：寻找一个函数值为0的参数