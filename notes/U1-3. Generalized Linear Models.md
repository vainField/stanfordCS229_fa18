# ***Part III Generalized Linear Models***

# **<u>L4. Perceptron. Exponential Family. Generalized Linear Models.</u>**

- So far, in the regression example, we had $y \mid x ; \theta \sim \mathcal{N}\left(\mu, \sigma^{2}\right)$, and in the classification one, $y \mid x ; \theta \sim \operatorname{Bernoulli}(\phi)$, for some appropriate definitions of $\mu$ and $\phi$ as functions of $x$ and $\theta$.
- In this section, we will show that both of these methods are special cases of a broader family of models, called **Generalized Linear Models (GLMs)**.^5^
  - ^5^ The presentation of the material in this section takes inspiration from Michael I. Jordan, *Learning in graphical models* (unpublished book draft), and also McCullagh and Nelder, *Generalized Linear Models* (2nd ed.).

## **Exponential Family**

- We say that a class of distributions is in the exponential family if it can be written in the form
  $$
  p(y ; \eta)=b(y) \exp \left(\eta^{T} T(y)-a(\eta)\right) \\ \\ \text{or written as} \\
  p(y ; \eta)={b(y) e^{\eta^{T} T(y)}\over e^{a(\eta)}}
  $$

  - $\eta$ is called the **natural parameter** (also called the **canonical parameter**) of the distribution; 
  - $T(y)$ is the **sufficient statistic** (for the distributions we consider, it will often be the case that $T(y)=y)$;
  - $b(y)$ is the **base measure**
  - $a(\eta)$ is the **log partition function**. 
    - The quantity $e^{-a(\eta)}$ essentially plays the role of <u>*a normalization constant*</u>, that makes sure the distribution $p(y ; \eta)$ sums/integrates over $y$ to 1 .
  - A fixed choice of $T, a$ and $b$ defines <u>*a family (or set) of distributions*</u> that is parameterized by $\eta$; as we vary $\eta$, we then get different distributions within this family.

- Properties

  1. Maximum Likelyhood Estimation (MSE) w.r.t. $\eta$ is **concave**, and Negative Log Likelihood (NLL) is **convex**
  2. $\mathrm{E}[y;\eta] = {\partial\over\partial\eta}a(\eta)$
  3. $\mathrm{Var}[y;\eta] = {\partial^2\over \partial \eta^2} a(\eta)$
     - Note that the expectation and variance computations include only derivatives and no integrals!

- We now show that the Bernoulli and the Gaussian distributions are examples of exponential family distributions.

  - There’re many other distributions that are members of the exponen- tial family: 
    - The multinomial; 
    - the Poisson (for modelling count-data); 
    - the gamma and the exponential (for modelling continuous, non-negative random variables, such as time- intervals); 
    - the beta and the Dirichlet (for distributions over probabilities); and many more.


### The Bernoulli distribution

- We write the Bernoulli distribution as:
  $$
  \begin{aligned}
  p(y ; \phi) &=\phi^{y}(1-\phi)^{1-y} \\
  &=\exp (y \log \phi+(1-y) \log (1-\phi)) \\
  &=\exp \left(\left(\log \left(\frac{\phi}{1-\phi}\right)\right) y+\log (1-\phi)\right)
  \end{aligned}
  $$

  - Thus, the natural parameter is given by $\eta=\log (\phi /(1-\phi))$. 

  - If we invert this definition for $\eta$ by solving for $\phi$ in terms of $\eta$, we obtain $\phi=$ $1 /\left(1+e^{-\eta}\right)$. This is the familiar sigmoid function!

  - To complete the formulation of the Bernoulli distribution as an exponential family distribution, we also have
    $$
    \begin{aligned}
    T(y) &=y \\
    a(\eta) &=-\log (1-\phi) \\
    &=\log \left(1+e^{\eta}\right) \\
    b(y) &=1
    \end{aligned}
    $$

    - This shows that the Bernoulli distribution can be written in the form of Equation (6), using an appropriate choice of $T, a$ and $b$.

### The Gaussian distribution

- When deriving linear regression, the value of $\sigma^{2}$ had no effect on our final choice of $\theta$ and $h_{\theta}(x)$​. To simplify the derivation below, let's set $\sigma^{2}=1$. We then have:
  $$
  \begin{aligned}
  p(y ; \mu) &=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2}(y-\mu)^{2}\right) \\
  &=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2} y^{2}\right) \cdot \exp \left(\mu y-\frac{1}{2} \mu^{2}\right)
  \end{aligned}
  $$

- Thus, we see that the Gaussian is in the exponential family, with
  $$
  \begin{aligned}
  \eta &=\mu \\
  T(y) &=y \\
  a(\eta) &=\mu^{2} / 2 \\
  &=\eta^{2} / 2 \\
  b(y) &=(1 / \sqrt{2 \pi}) \exp \left(-y^{2} / 2\right)
  \end{aligned}
  $$

## **Generalized Linear Models (GLMs)**

- In this section, we will we will describe a method for constructing GLM models for problems

- Generally, consider a classification or regression problem where we would like to predict the value of some random variable $y$ as a function of $x$. To derive a GLM for this problem, we will make the following three assumptions about the conditional distribution of $y$ given $x$ and about our model:

  1. $y \mid x ; \theta \sim$ ExponentialFamily $(\eta)$. I.e., given $x$ and $\theta$, the distribution of $y$ follows some exponential family distribution, with parameter $\eta$.

  2. Given $x$, our goal is to predict the expected value of $T(y)$ given $x$. In most of our examples, we will have $T(y)=y$, so this means we would like the prediction $h(x)$ output by our learned hypothesis $h$​ to satisfy $h(x)=\mathrm{E}[y \mid x]$. 

     - Note that this assumption is satisfied in the choices for $h_{\theta}(x)$ for both logistic regression and linear regression. 

     - For instance, in logistic regression, we had 

       $\begin{align}h_{\theta}(x)&=p(y=1 \mid x ; \theta)\\&=0 \cdot p(y=0 \mid x ; \theta)+1 \cdot p(y=1 \mid x ; \theta)\\&=\mathrm{E}[y \mid x ; \theta] \end{align}$

  3. The natural parameter $\eta$ and the inputs $x$ are related linearly: $\eta=\theta^{T} x$. (Or, if $\eta$ is vector-valued, then $\eta_{i}=\theta_{i}^{T} x$)

     - This last assumption might seem the least well justified of the above, and it might be better thought of as a “design choice” in our recipe for designing GLMs, rather than as an assumption per se.

- These three assumptions/design choices will allow us to <u>*derive a very elegant class of learning algorithms*</u>, namely GLMs, that have many desirable properties such as ease of learning.

- Furthermore, the resulting models are often *<u>very effective for modelling different types of distributions over $y$;</u>* for example, we will shortly show that both logistic regression and ordinary least squares can both be derived as GLMs.

### Ordinary Least Squares

- Consider the setting where the target variable $y$ (also called the **response variable** in GLM terminology) is continuous, and we model the conditional distribution of $y$ given $x$ as as a Gaussian $\mathcal{N}\left(\mu, \sigma^{2}\right)$. (Here, $\mu$ may depend $x$.) We have
  $$
  \begin{aligned}
  h_{\theta}(x) &=E[y \mid x ; \theta] \\
  &=\mu \\
  &=\eta \\
  &=\theta^{T} x
  \end{aligned}
  $$

  - The first equality follows from Assumption 2, above; 
  - the second equality follows from the fact that $y \mid x ; \theta \sim \mathcal{N}\left(\mu, \sigma^{2}\right)$, and so its expected value is given by $\mu$; 
  - the third equality follows from Assumption 1 (and our earlier derivation showing that $\mu=\eta$ in the formulation of the Gaussian as an exponential family distribution); 
  - and the last equality follows from Assumption 3 .

### Logistic Regression

- Here we are interested in binary classification, so $y \in\{0,1\}$. Given that $y$ is binary-valued, it therefore seems natural to choose the Bernoulli family of distributions to model the conditional distribution of $y$ given $x$. 

  - In our formulation of the Bernoulli distribution as an exponential family distribution, we had $\phi=1 /\left(1+e^{-\eta}\right)$. 
  - Furthermore, note that if $y \mid x ; \theta \sim \operatorname{Bernoulli}(\phi)$, then $\mathrm{E}[y \mid x ; \theta]=\phi$. 

- So, following a similar derivation as the one for ordinary least squares, we get:
  $$
  \begin{aligned}
  h_{\theta}(x) &=E[y \mid x ; \theta] \\
  &=\phi \\
  &=1 /\left(1+e^{-\eta}\right) \\
  &=1 /\left(1+e^{-\theta^{T} x}\right)
  \end{aligned}
  $$

- To introduce a little more terminology, the function $g$ giving the distribution's mean as a function of the natural parameter $(g(\eta)=\mathrm{E}[T(y) ; \eta])$ is called the **canonical response function**. Its inverse, $g^{-1}$, is called the **canonical link function**. 

  - Thus, the canonical response function for the Gaussian family is just the identity function; and the canonical response function for the Bernoulli is the logistic function.

## **Softmax Regression**

- Consider a classification problem in which the response variable $y$ can take on any one of $k$ values, so $y \in$ $\{1,2, \ldots, k\}$. We will thus model it as distributed according to a multinomial distribution.

  - We will parameterize the multinomial with only $k-1$ parameters, $\phi_{1}, \ldots, \phi_{k-1}$, where $\phi_{i}=p(y=i ; \phi)$, and $p(y=k ; \phi)=1-\sum_{i=1}^{k-1} \phi_{i}$. For notational convenience, we will also let $\phi_{k}=1-\sum_{i=1}^{k-1} \phi_{i}$, but we should keep in mind that this is not a parameter

  - To express the multinomial as an exponential family distribution, we will define $T(y) \in \mathbb{R}^{k-1}$ as follows:
    $$
    T(1)=\left[\begin{array}{c}
    1 \\
    0 \\
    0 \\
    \vdots \\
    0
    \end{array}\right], T(2)=\left[\begin{array}{c}
    0 \\
    1 \\
    0 \\
    \vdots \\
    0
    \end{array}\right], T(3)=\left[\begin{array}{c}
    0 \\
    0 \\
    1 \\
    \vdots \\
    0
    \end{array}\right], \cdots, T(k-1)=\left[\begin{array}{c}
    0 \\
    0 \\
    0 \\
    \vdots \\
    1
    \end{array}\right], T(k)=\left[\begin{array}{c}
    0 \\
    0 \\
    0 \\
    \vdots \\
    0
    \end{array}\right],
    $$

    - We will write $(T(y))_{i}$ to denote the $i$-th element of the vector $T(y)$.
    - We can also write the relationship between $T(y)$ and $y$ as $(T(y))_{i}=1\{y=i\}$.
    - Further, we have that $\mathrm{E}\left[(T(y))_{i}\right]=P(y=i)=\phi_{i}$.

- We are now ready to show that the multinomial is a member of the exponential family. We have:
  $$
  \begin{aligned}
  p(y ; \phi) &=\phi_{1}^{1\{y=1\}} \phi_{2}^{1\{y=2\}} \cdots \phi_{k}^{1\{y=k\}} \\
  &=\phi_{1}^{1\{y=1\}} \phi_{2}^{1\{y=2\}} \cdots \phi_{k}^{1-\sum_{i=1}^{k-1} 1\{y=i\}} \\
  &=\phi_{1}^{(T(y))_{1}} \phi_{2}^{(T(y))_{2}} \cdots \phi_{k}^{1-\sum_{i=1}^{k-1}(T(y))_{i}} \\
  &=\exp \left((T(y))_{1} \log \left(\phi_{1}\right)+(T(y))_{2} \log \left(\phi_{2}\right)+\right.\\
  &=\exp \left((T(y))_{1} \log \left(\phi_{1} / \phi_{k}\right)+(T(y))_{2} \log \left(\phi_{2} / \phi_{k}\right)+\right.\\
  &\left.\cdots+(T(y))_{k-1} \log \left(\phi_{k-1} / \phi_{k}\right)+\log \left(\phi_{k}\right)\right) \\
  &=b(y) \exp \left(\eta^{T} T(y)-a(\eta)\right)
  \end{aligned}
  $$
  ​	where
  $$
  \begin{aligned}
  \eta &=\left[\begin{array}{c}
  \log \left(\phi_{1} / \phi_{k}\right) \\
  \log \left(\phi_{2} / \phi_{k}\right) \\
  \vdots \\
  \log \left(\phi_{k-1} / \phi_{k}\right)
  \end{array}\right] \\
  a(\eta) &=-\log \left(\phi_{k}\right) \\
  b(y) &=1
  \end{aligned}
  $$

  - The <u>link function</u> is given (for $i=1, \ldots, k$ ) by
    $$
    \eta_{i}=\log \frac{\phi_{i}}{\phi_{k}}
    $$

  - For convenience, we have also defined $\eta_{k}=\log \left(\phi_{k} / \phi_{k}\right)=0$. To invert the link function and derive the response function, we therefore have that
    $$
    \begin{aligned}
    e^{\eta_{i}} &=\frac{\phi_{i}}{\phi_{k}} \\
    \phi_{k} e^{\eta_{i}} &=\phi_{i} &\text{(7)}\\
    \phi_{k} \sum_{i=1}^{k} e^{\eta_{i}} &=\sum_{i=1}^{k} \phi_{i}=1
    \end{aligned}
    $$

  - This implies that $\phi_{k}=1 / \sum_{i=1}^{k} e^{\eta_{i}}$, which can be substituted back into Equation $(7)$ to give the <u>response function</u>
    $$
    \phi_{i}=\frac{e^{\eta_{i}}}{\sum_{j=1}^{k} e^{\eta_{j}}}
    $$

  - This function mapping from the $\eta$ 's to the $\phi$ 's is called the **softmax function**.

- With assumption 3, we have $\eta_{i}=\theta_{i}^{T} x$ (for $i=1, \ldots, k-1$ ), where $\theta_{1}, \ldots, \theta_{k-1} \in \mathbb{R}^{n+1}$ are the parameters of our model.

  - For notational convenience, we can also define $\theta_{k}=0$, so that $\eta_{k}=\theta_{k}^{T} x=0$, as given previously.

- Hence, our model assumes that the conditional distribution of $y$ given $x$ is given by
  $$
  \begin{aligned}
  p(y=i \mid x ; \theta) &=\phi_{i} \\
  &=\frac{e^{\eta_{i}}}{\sum_{j=1}^{k} e^{\eta_{j}}} \\
  &=\frac{e^{\theta_{i}^{T} x}}{\sum_{j=1}^{k} e^{\theta_{j}^{T} x}}
  \end{aligned}
  $$

  - This model, which applies to classification problems where $y \in\{1, \ldots, k\}$, is called **softmax regression**. 
    - It is <u>*a generalization of logistic regression*</u>.

- Our hypothesis will output
  $$
  \begin{aligned}
  h_{\theta}(x)=& \mathrm{E}[T(y) \mid x ; \theta] \\
  =&\mathrm{E}\left[\left.\begin{array}{c}
  1\{y=1\} \\
  1\{y=2\} \\
  \vdots \\
  1\{y=k-1\}
  \end{array} \right| x ; \theta\right] \\
  =&\left[\begin{array}{c}
  \phi_{1} \\
  \phi_{2} \\
  \vdots \\
  \phi_{k-1}
  \end{array}\right] \\
  =&\left[\begin{array}{c}
  \frac{\exp \left(\theta_{1}^{T} x\right)}{\sum_{j=1}^{k} \exp \left(\theta_{j}^{T} x\right)} \\
  \frac{\exp \left(\theta_{2}^{T} x\right)}{\sum_{j=1}^{k} \exp \left(\theta_{j}^{T} x\right)} \\
  \vdots \\
  \frac{\exp \left(\theta_{k-1}^{T} x\right)}{\sum_{j=1}^{k} \exp \left(\theta_{j}^{T} x\right)}
  \end{array}\right]
  \end{aligned}
  $$

  - In other words, our hypothesis
  - will output the estimated probability that $p(y=i \mid x ; \theta)$, for every value of $i=1, \ldots, k$.

- Lastly, let's discuss parameter fitting. Similarly, we would begin by writing down the log likelihood
  $$
  \begin{aligned}
  \ell(\theta) &=\sum_{i=1}^{m} \log p\left(y^{(i)} \mid x^{(i)} ; \theta\right) \\
  &=\sum_{i=1}^{m} \log \prod_{l=1}^{k}\left(\frac{e^{\theta_{l}^{T} x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{T} x^{(i)}}}\right)^{1\left\{y^{(i)}=l\right\}}
  \end{aligned}
  $$

  - We can now obtain the maximum likelihood estimate of the parameters by maximizing $\ell(\theta)$ in terms of $\theta$, using a method such as gradient ascent or Newton's method.

## *Thinking*

- 由于指数族的一些有利于机器学习的特征：MSE和NLL分别是凹函数和凸函数^1^等

  ^1^ （注意：中国大陆数学界某些机构关于函数凹凸性定义和国外的定义是相反的）

- ==》我们假设 $y \mid x ; \theta$ 的分布属于指数族

- ==》由此通过数学推导得出了机器学习（线性回归）中，回归问题、二分类问题、多分类问题的假设函数（hypothesis）分别为：线性方程（Affine？）、Sigmoid、Softmax